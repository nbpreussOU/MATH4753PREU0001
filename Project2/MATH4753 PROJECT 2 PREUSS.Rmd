---
title: "MATH 4753 Project 2"
author: "Nathan Preuss"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    csl: biomed-central.csl
    df_print: paged
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    toc: yes
    toc_depth: 4
bibliography: project.bib
abstract: This project is all about applying Simple Linear Regresssion (SLR) to real data using R.  In this case, SLR will be used to analyze a data set showing the realtionship between the number of deaths from a certain disease and the amount of money donated to charities committed to fighting that disease.  I will accomplish this by giving an introduction to the data set, attempting to validate the assumptions of SLR, analyzing the model, and then using the model to make predictions for other diseases.
---

# Introduction

I am trying to determine the relationship between the number of deaths caused by a disease and the amount donated to charities who are fighting that same disease.  In this section I will discuss the data, its variables, how the data was constructed, potential sampling bias, and why I chose this for my project.  

## What are the variables? 

Listed below is a table of the diseases and the charities involved in fighting that disease.  The list of diseases is a categorical variable, and the list of charities is categorical as well.  Some diseases have multiple charities fighting that disease, and other diseases only have 1 charity.

### Table 1: Diseases and the corresponding charities

<center>

| Disease | Charity |
| ------ | -------|
| Brain Cancer  | American Brain Tumor Association  |
| Kidney Disease  | American Kidney Fund  |
|   | National Kidney Foundation  |
|   | NephCure Kidney International  |
|   | PKD Foundation  |
|   Liver Disease | American Liver Foundation |
| Lung Cancer | LUNGevity Foundation | 
|  | Lung Cancer Research Foundation |
| Lung Disease | American Lung Foundation | 
| Colorectal Cancer | Crohn's and Colitis Foundation | 
| Parkinson's Disease | Michael J. Fox Foundation for Parkinson's Research | 
| Breast Cancer | Breast Cancer Prevention Partners | 
| | Breast Cancer Research Foundation | 
|  | National Breast Cancer Coalition Fund | 
| Prostate Cancer | Prostate Cancer Foundation | 
| Leukemia | Leukemia and Kymphoma Society | 
| Multiple Myeloma | Multiple Myeloma Research Foundation| 
| Ovarian Cancer | Ovarian Cancer Research Allinace | 
| Tuberculosis | TB Alliance | 
| Pancreatic Cancer | Pancreatic Cancer Action Network|
| | The Lustgarten Foundation for Pancreatic Cancer Research | 
|Alzheimer's Disease | Alzheimer's Association | 
| | Cure Alzheimer's Fund |
| Bladder Cancer | Bladder Cancer Advocacy Network |
| Thyroid Cancer | THANC Foundation | 

</center>

### Table 2: An interactive data table 

This table highlights the amount of donations and the number of deaths from a given disease while displaying the full data set.  The amount of donations (in dollars) and the number of deaths are discrete variables, and the percentage of deaths is a continuous variable.

```{r donataions}
library(readxl)
MATH4753Data <- read_excel("MATH4753Data.xlsx")
df <- as.data.frame(MATH4753Data)

library(DT)
datatable(
  df,filter = 'top', options = list(
  pageLength = 5, autoWidth = TRUE, editable = TRUE, dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel', 'pdf', 'print')),
caption = htmltools::tags$caption(
    style = 'caption-side: bottom; text-align: center;',
    'Table 2: ', htmltools::em('Project 2 Interactive Data Table.')
  )
) %>%
  formatStyle('Donations',
  background = styleColorBar(range(df$Donations), 'lightgreen'),
  backgroundSize = '98% 88%',
  backgroundRepeat = 'no-repeat',
  fontWeight = 'bold',
  backgroundPosition = 'center')%>%
  formatStyle('NumberDeaths',
  background = styleColorBar(range(df$NumberDeaths), 'lightblue'),
  backgroundSize = '98% 88%',
  backgroundRepeat = 'no-repeat',
  fontWeight = 'bold',
  backgroundPosition = 'center')
```

In the background of the Donations and NumberDeaths columns, colored bars indicate the amount of money donated and the number of deaths relative to the maximum number in each category. Most of the donations are relatively small, except for three charities that received copious amounts of money.

### Figure 1

This is a simple plot of the data points for this project, color coded by disease.

```{r}
library(ggplot2)

# Number 1
x<-ggplot(df, aes(x=NumberDeaths, y=Donations)) + geom_point(aes(color = factor(Disease))) + ggtitle("Figure 1: Number of Deaths versus Donations given Disease")
x
```

By looking at Figure 1, an obvious best fit line through the data does not exist.  The data is rather noisy, and some outlier will probably have to be removed in order to reduce some of the noise.

## Where was the data collected?

I collected the data from freely available data sets from CharityWatch, Charity Navigator, and the CDC (see below).

## How were the data collected? 

I went to 3 different websites to gather different data on charities and death rates.  CharityWatch [@charitywatch_2020] and Charity Navigator [@charitynavigator_2020] gave me information about the financial statements from different charities, specifically about how much money they took in donations.  I took information about the death rates for specific diseases from the CDC [@cdc_2015] and matched charities that focused on a specific cause of death (i.e. lung cancer, not cancer in general) to the number of people who died from that cause of death.  

### Assumption about the data

There are some assumptions about the sampling of the data that may effect the regression analysis.  While I cannot correct for them here, perhaps some future research will be able to address these.

1. Each charity is equally effective.  For example, the data assumes that \$1 donated to breast cancer won't save more lives than \$1 to fight Kidney Disease.  If this were not the case, then people might donate to a specific charity because they were particularly effective at saving lives, rather than they wanted to prevent others from dying from the same disease as their loved one.

2. The charities for which data are gathered represent the same proportion of donations for a particular disease. That is to say, the breast cancer charities listed in the data set shouldn't represent only 30% of the total amount of donations to fight breast cancer while the pancreatic cancer charities listed in the data set represent 80% of the total donations to fight pancreatic cancer. The regression analysis is assuming that the proportion of donations to each disease is constant across different diseases.

3. Sampling Bias.  There is a lack of comprehensive data for charities, especially for the amount of money donated to specific charities. The data that was freely available may or may not be representative of the population of charities.

4. Time.  The latest death statistics are from 2015, but the charity financial statements are mostly from 2018, 2019, and 2020.  I am assuming that the same number of people died of the same causes of death each year.  

## What is your interest in the data?

<center>
![Doing Good Better [@macaskill_2016]](dgb.JPG){width=30%}

</center>

As I was reading *Doing Good Better*, by William Macaskill, I wanted to gather some data to test the author's claim that most people donate to charities ineffectively.  He claims that most people donate to charities whose cause they believe in or to prevent others from dying in the same way as their loved one; instead of donating to the charities that can maximize the number of lives saved [@macaskill_2016].  If most people acted this way, then I would expect to see more money donated to charities that try and fight diseases that cause more deaths than diseases that cause fewer deaths.  This area didn't appear to have much original research, and so I was curious to see if I could prove or disprove that hypothesis.

## What is the story behind the data?

All deaths are equal, but some deaths are more equal than others.  If a loved one dies from a preventable disease, it seems like a reasonable assumption that their kin might donate to charity to try and save others from the disease.  If more people die from a given disease, then more donations should go to fight that disease.  I wanted to test to see if this was really the case, so I accumulated the data.

## What problem do you wish to solve?

If people are donating to charities ineffectively, as Macaskill argues, then  more lives can be saved in the future without increasing the amount of money donated to charities.  However, it first needs to be proven that people are donating to charities ineffectively.  All deaths being equal, I would expect to see a linear relationship between the number of deaths and the amount of donations toward charities because people donate inefficiently.  

# Theory needed to carry out SLR

## Background for Choosing SLR
I think that the amount of donations (y) tends to increase linearly as the number of deaths of a disease (x) increases, with the amount of donations being the dependent variable, and the number of deaths as the independent variable. From looking at the preliminary graph of the points above, I know that the data points are quite noisy, such that it might be easier to use the data points to form a constellation rather than fit a linear model.  As a result, I will have to use a probabilistic model in my analysis, and simple linear regression (SLR) is the one I will use in my analysis.  The following theory behind SLR comes from the textbook [@mendenhall_sincich_2016].

## Assumptions of SLR

SLR assumes that the mean value of the y data, for any given value for the x data, will end up making a straight line when graphed.  Any points that deviate above or below this line have a deviation equal to $\epsilon$.  For a function with one x variable, the resulting SLR function can be written as:
$$y = \beta_0 + \beta_1x_i + \epsilon_i$$
Where $\beta_0$ and $\beta_1$ are currently unknown coefficients, $\beta_0 + \beta_1x_i$ is the mean value of y for any given $x_i$, and $\epsilon$ is the random error.

In order to use SLR, several assumptions need to be validated about $\epsilon$.  

* Because we know that the data points will deviate from the average, either higher or lower, they will have an accompanying error $\epsilon$ to some extent.  However, taken across the entire data set, we can expect the mean of the probability distribution of $\epsilon$ to be  $0$. 
* Furthermore, we can expect that the variance of the probability distribution of $\epsilon$ is equivalent to a constant for all values for x, such that $V(\epsilon) = c$ for some constant $c$.  Often, this constant is $\sigma^2$.
* In addition, we are assuming that the probability distribution of $\epsilon$ is a normal distribution.
* And finally, the errors associated with different data points are independent - the error of one data point has no bearing on the error from a different data point.

Using these assumptions, we can rewrite the SLR equation from above to be more useful:

$$E(y) = E(\beta_0 + \beta_1x_i + \epsilon_i)$$
$$E(y) = \beta_0 + \beta_1x_i + E(\epsilon_i)$$
$$E(y) = \beta_0 + \beta_1x_i + 0$$
$$E(y) = \beta_0 + \beta_1x_i$$

Thus, the mean value of $y$ for any given value of $x$ will graph as a straight line, with a y-intercept of $\beta_0$ and a slope of $\beta_1$.

# Creating and Verifying Models
I will be creating 2 different models, a linear model and and exponential model.  To create my exponential model, I will be taking the natural logarithm of the amount of money donated to charities and appending that as another column on my dataframe, and then I will create a linear model where the Y-axis has a logarithmic scale. 

```{r}
# create the new colum
library(SciViews)
lnVals <- ln(df$Donations)
df = cbind(df, lnVals)

deaths.lm <- with(df, lm(Donations~NumberDeaths))
deaths.exp <- with(df, lm(lnVals~NumberDeaths))
```

The deaths.lm model is testing a linear relationship between the variables, and the deaths.exp model is testing a multiplicative relationship between the variables.

## Checks on validity

I am going to preform multiple checks for validity for both of the models.

### Errors distributed Normally

First, I am going to check that the errors are distributed normally, such that $\epsilon_i \sim N(0,\sigma^2)$.  To do this, I will do a Shapiro-wilk test for both models.

#### Shapiro-wilk Test on the Linear Model

```{r}
library(s20x)
normcheck(deaths.lm, shapiro.wilk = TRUE)
```

The P-value is less than $0.05$, so the null hypothesis that the errors are distributed normally is rejected.  This means that this model fails one of the basic assumptions of SLR, and will need to be modified for continued analysis.  Perhaps there are a few outliers that warrant further investigation.

##### Using Cook's Distance to Investigate Outliers in the Linear Model

To identify potential outliers, I will create a Cook's plot to determine which data points have a high Cook's distance.  Higher Cook's distance means that the specified data point has an outsized effect on the regression analysis.  

```{r}
library(olsrr)
ols_plot_cooksd_chart(deaths.lm)
```

Observations 4, 5, and 15 all have significantly high Cook's distances.  Removing data points with high Cook's distances is one way to correct for their impact.  I will start by removing the observation with the highest Cook's distance and see how that effects the cooks plot

```{r}
#removing the larges
#copying the data into a new data frame to that the exponential model isn't affected by these removal

#remove observation 4
df.removed1 <- df[-4, ]
deaths.lm.removed1 <- with(df.removed1, lm(Donations~NumberDeaths))
ols_plot_cooksd_chart(deaths.lm.removed1)
```

After removing observation 4, we see that the new observation 4 (the old observation 5) still has a extremely large cooks distance. I am going to correct for that by removing that observation from the data set.

```{r}
#removing the largest observation

#remove observation 4
df.removed2 <- df.removed1[-4, ]
deaths.lm.removed2 <- with(df.removed2, lm(Donations~NumberDeaths))
ols_plot_cooksd_chart(deaths.lm.removed2)
```

While there are still observations with high Cook's distances, they seem much more reasonable than the previous two Cook's distance plots.  Let's re-run the Shapiro-wilks test to see if the p-value is much greater than .05.

```{r}
normcheck(deaths.lm.removed2, shapiro.wilk = TRUE)
```

In this case, the P-value is 0.395, much higher than .05.  As a result, I will use the modified model with two removed data points in place of the original model because it was able to validate the first assumption of SLR.

#### Shapiro-wilk Test on the Exponential Model

```{r}
normcheck(deaths.exp, shapiro.wilk = TRUE)
```
The P-value (.099) is greater than $0.05$ (though not by much), so the null hypothesis that the errors are distributed normally is accepted.  

### Constant Variance

With both models passing the Shapiro-wilks test, next I am going to look to see if the models have constant variance.  To do that, I will be creating residual vs fitted plots for both the linear and the exponential model.

#### Residual vs Fitted Values for the Linear Model

I will be creating a trendscatter plot on the residual and fitted values for the linear model. 
```{r}
linear.res <- residuals(deaths.lm.removed2)
linear.fit <- fitted(deaths.lm.removed2)
trendscatter(linear.res~linear.fit, f=0.5, data=df.removed2, main="Linear Model Residuals vs. Fitted with f=0.5")
```

Well, the residuals vs the fitted aren't a horizontal line, and instead they look like a positive quadratic equation. As a result, I am going to add a quadratic term to the linear model and see if that addresses the issues with the residuals vs fitted.

```{r}
quad.lm <- with(df.removed2, lm(Donations~NumberDeaths + I(NumberDeaths^2)))
quad.res <- residuals(quad.lm)
quad.fit <- fitted(quad.lm)
trendscatter(quad.res~quad.fit, f=0.5, data=df.removed2, main="Quadratic Model Residuals vs. Fitted with f=0.5")
```

The quadratic model did not drastically effect the residuals/fitted plot.  As a result, the Linear Model fails to validate the assumptions of SLR, and more data should be collected to attempt to fix these errors.  After talking to Dr. Stewart, I will continue to analyze this model as if it validated the assumptions of SLR, but I will point out later that the conclusions drawn aren't supported by the data.  To make things easier, I am going to move forward analyzing the linear model, since the quadratic did not result in any improvements over the linear model.

#### Residual Vs Fitted for the Exponential Model

I will be creating a trendscatter plot on the residual and fitted values for the exponential model. 

```{r}
exp.res <- residuals(deaths.exp)
exp.fit <- fitted(deaths.exp)
trendscatter(exp.res~exp.fit, f=0.5, data=df, main="Exponential Model Residuals vs. Fitted with f=0.5")
```

There is no recognizable pattern in the residual vs. fitted for this plot.  As a result, the exponential model validates this assumption. However, the noisiness of the model ensures that the residuals vs fitted is far from being a straight line at 0.

### Zero mean value of $\epsilon$ for both models.

Next, I am going to analyze the residuals ($\epsilon$) of both the linear and the exponential model to check if their mean is 0.

```{r}
summary(residuals(deaths.lm.removed2))
summary(residuals(deaths.exp))
```

Both models have a 0 mean value for $\epsilon$.

### Independence of data for both models

Since both models have only one independent variable, I cannot check for covariance between the independent variables.  Thus, I am going to assume that both models have independent data.

# Model selection

I will now be comparing the models to see which one better represents the data.  Based on the assumptions of validity, I should go forward analyzing the exponential model since it validated all of the assumptions for SLR.

## Use adjusted $R^2$ to compare models

I will be using adjusted R-squared to compare the two models  The R-squared for a given model represents how well it fits the particular data, while adjusted R-square values offer a way to compare the fit of two different models.  R-square values close to 1 means that the model fits the data well, and scores near 0 means that the model does not fit the data well.

```{r}
summary(deaths.lm.removed2)
summary(deaths.exp)
```

I have never seen an adjusted R-squared value of 0.0194, which I wasn't expecting.  So I have a choice between a model that fails to validate the assumptions of SLR but has a statistically significant variable, and a model that validates the assumptions of SLR but provides little fit to the data.

If you remember back, I had to remove a couple of outliers for the linear model, and I did not remove them for the exponential model.  So I'm going to check to see if the outliers make a difference on the exponential model.

### Using Cook's Distance to investigate outliers again

I will be using Cook's distance to identify any potential outliers, and then remove them from the model to account for them.

```{r}
library(olsrr)
ols_plot_cooksd_chart(deaths.exp)
```

I think removing data point 4 from the data set would be a great idea, given it's high Cook's Distance.

```{r}
#remove observation 4
df.removed.exp <- df[-4, ]
deaths.exp.removed <- with(df.removed.exp, lm(lnVals~NumberDeaths))
ols_plot_cooksd_chart(deaths.exp.removed)
```

It appears that there is still an outlier, the new data point 4 is acting as an outlier.

```{r}
#remove observation 4 again
df.removed.exp1 <- df.removed.exp[-4, ]
deaths.exp.removed1 <- with(df.removed.exp1, lm(lnVals~NumberDeaths))
ols_plot_cooksd_chart(deaths.exp.removed1)
```

This Cook's Distance plot is much more reasonable.  Having altered the data, I will quickly retest this model to see if it meets the core assumptions for SLR.

```{r}
# Shapiro-wilks
library(s20x)
normcheck(deaths.exp.removed1, shapiro.wilk = TRUE)

# residuals vs fitted
exp1.res <- residuals(deaths.exp.removed1)
exp1.fit <- fitted(deaths.exp.removed1)
trendscatter(exp1.res~exp1.fit, f=0.5, data=df, main="Exponential Model Residuals vs. Fitted with f=0.5")

summary(residuals(deaths.exp.removed1))
```

For the shapiro-wilks test, this model has a p-vale of 0.115, which is greater than .05, which means that the null hypothesis is accepted and the errors are distributed normally.

For the residual vs. fitted test, there is a sinusoidal pattern in the data, so it does not validate the assumptions of SLR.

The mean value of the errors is 0, meaning that this model validates the third assumption.

And, the data is still independent.

## Using Adjusted R-Squared to compare models, take 2

Now that both models do not validate the assumptions of SLR, I will analyze whichever model has a higher adjusted R-square.

```{r}
summary(deaths.lm.removed2)
summary(deaths.exp.removed1)
```

The exponential model did improve it's R-squared, up to .2196, but that is still less than the linear model.  As a result, I am going to analyze the results from the linear model, bearing in mind that the conclusions drawn cannot be accepted.

# Analysis of the Linear Model

In this section I am going to analyze the results of the linear model developed in the previous section.

## Basic Plot of Number of Deaths vs. Donations

Let's look another look at the data after the outliers have been removed.

```{r}
library(ggplot2)
df.final <- df.removed2[,-6]
x<-ggplot(df.final, aes(x=NumberDeaths, y=Donations)) + geom_point(aes(color = factor(Disease))) + ggtitle("Figure 2: Number of Deaths versus Donations given Disease")
x
```

It appears that the two outliers that we removed were in the bottom right hand corner.  

## Add the trend to the data

Now, I am going to add the trend line to the plot.

```{r}
with(df.final,
plot(Donations~NumberDeaths,bg="Blue",pch=21,ylim=c(0,1.1*max(Donations)),xlim=c(0,1.1*max(NumberDeaths))) + title("Figure 3: of Deaths vs Donations with a Best Fit Line")
)
abline(deaths.lm.removed2)
```

It appears that with each additional death, the amount of donations increases.

## Interpretation of the summary of the model

```{r}
summary(deaths.lm.removed2)
```

Here is the summary of the linear model.  Using the data provided in the summary, I can find the $\beta_0$ and the $\beta_1$ values.
In this case, $\beta_0 = 2,073,000$ and $\beta_1 = 2,894$, thus giving us the equation $y = 2894x + 2073000$.  In other words, for each addition death, a charity on average receives another \$2,894, and each charity about \$2 million in baseline funding. 

This model has a multiple R-squared of .4734, meaning that the model does an adequate job of describing the data.

## Calculating Confidence Intervals for $\beta$ parameter estimates

Next, I will calculate the 95% confidence intervals for $\beta_0$ and $\beta_1$.

```{r}
confint(deaths.lm.removed2)
```

The 95% confidence interval for the $\beta_0$ is quite large, falling between -75,669,882 and 79,816,096, which is quite a large span of numbers.  The confidence interval for $\beta_1$ is 1144 to 4643.  This means that the baseline funding for an organization falls in a wide range of possible values, including several values that aren't feasible, as it makes little sense for a charity to have negative amounts of funding.  On the other hand, the confidence interval for $\beta_1$ indicates that 95% of the time charities would receive between \$1144 and \$4643 per additional death.

### Graph With Confidence Intervals

I will now graph the best fit line with confidence intervals.

```{r}
library(ggplot2)
g <- ggplot(df.final, aes(x=NumberDeaths, y=Donations)) +geom_point() +geom_smooth(method=lm , color="red", fill="#69b3a2", se=TRUE) + ggtitle("Figure 4: Plot With Confidence Intervals")
g
```

Because of the noisiness of the data, there is the potential for a wide variety of best fit lines.

### Using the model to make Predictions

Next, I am going to predict the amount of donations a charity could based on a given number of deaths.

```{r}
predict(deaths.lm.removed2, data.frame(NumberDeaths=c(15000,60000,120000)))
```

According to the predictions, charities fighting a disease causing 15,000 deaths would expect a little over \$45 million in donations, charities fighting a disease that causes 60,000 deaths would expect to receive about a bit more than \$175 million in donations, and a disease causing 120,000 deaths would yield about \$350 million in donations.

# Conclusion

The following conclusion is not valid because the model does not validate the assumptions of SLR.  In conclusion, the amount of donations a charity receives is dependent upon the number of people who die from the disease the charity is fighting.  This relationship is a constant linear relationship, with each additional death generating a constant amount of donations.  On average, charities will receive about \$2,894 per additional death to a disease.  In addition, I found that charities get about \$2,073,000 for simply existing, or fighting a disease that causes 0 deaths. Therefore, it is possible that people donate money to charities ineffectively, lending credence to Macaskill's claims. 

# Future Research

There are many things that can be done to improve this model:

1. Gather more and better data.  There was a distinct lack of good data for the project, and gathering more data would help correct for the issues in the residuals vs. fixed graph.
2. Address the sampling concerns I listed earlier in the paper.  Gathering more data, and looking at how the data changes over the years are other ways to expand this research project.
3. Determine other factors that may determine donation rates.  There was only one factor considered in the model, and it had fairly poor predictive power.  Gathering a few more variables may help increase the predictive power of this model.

# References
  
